# Naive Fine-tuning Configuration Example
# This is the simplest fine-tuning approach without any regularization

model:
  architecture: nequip
  package_path: /path/to/pretrained_model.nequip.zip
  r_max: 6.0
  compile_mode: eager

data:
  finetune_data: /path/to/finetune_data.xyz
  batch_size: 8
  train_split: 0.8
  val_split: 0.2
  test_split: 0.0

training:
  epochs: 100
  lr: 1e-4
  optimizer: adam
  scheduler:
    type: ReduceLROnPlateau
    patience: 10
    factor: 0.5
    min_lr: 1e-6
  early_stopping:
    patience: 20
    min_delta: 1e-6

loss_coeffs:
  energy: 0.0       # Weight for energy loss
  force: 100.0      # Weight for force loss  
  stress: 0.0       # Weight for stress loss (set > 0 to include)

# Metrics to track during training (computed for train/val/test sets)
# These are logged to wandb and tensorboard
metrics:
  - energy_mae
  - force_mae
  - force_cosine

strategy:
  name: naive

logging:
  tensorboard: true
  log_interval: 1
  save_checkpoint_interval: 10

device: cuda
seed: 42
deterministic: true

