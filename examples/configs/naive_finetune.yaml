# Naive Fine-tuning Configuration Example
# This is the simplest fine-tuning approach without any regularization

model:
  architecture: nequip
  package_path: /DATA/user_scratch/pn50212/2025/01_Moire/Finetuning/NequIP-OAM-L-0.1.nequip.zip
  r_max: 6.0
  compile_mode: eager

data:
  finetune_data: /DATA/user_scratch/pn50212/2025/01_Moire/Finetuning/mlip-finetune/mlip_finetune/test_data/bto_100.xyz
  batch_size: 4
  train_split: 0.8
  val_split: 0.2
  test_split: 0.0

training:
  epochs: 3
  lr: 1e-4
  optimizer: adam
  scheduler:
    type: ReduceLROnPlateau
    patience: 10
    factor: 0.5
    min_lr: 1e-6
  early_stopping:
    patience: 20
    min_delta: 1e-6

loss_coeffs:
  energy: 0.0       # Weight for energy loss
  force: 100.0      # Weight for force loss  
  stress: 0.0       # Weight for stress loss (set > 0 to include)

# Metrics to track during training (computed for train/val/test sets)
# These are logged to wandb and tensorboard
metrics:
  - energy_mae
  - force_mae
  - force_cosine

strategy:
  name: naive

logging:
  tensorboard: true
  save_checkpoint_interval: 10  # Save checkpoint every N epochs
  
  # Parity plots (energy & force)
  parity_plots:
    enabled: true
    save_interval: 1  # Generate plots every N epochs
    plot_energy: true
    plot_force: true
    per_atom_energy: true  # Use per-atom energy for plotting
  
  # Weights & Biases (optional)
  wandb:
    enabled: true
    project: "test_mlip-finetune"
    name: test_naive  # Auto-generated if not specified
    entity: null  # null = use default from login (snupark-)
    tags: ["naive"]
    log_model: false  # Whether to save model as wandb artifact

device: cuda
seed: 42
deterministic: true

